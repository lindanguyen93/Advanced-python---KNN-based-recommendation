{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and load necesary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please don't change this cell\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>298</td>\n",
       "      <td>474</td>\n",
       "      <td>4</td>\n",
       "      <td>884182806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>115</td>\n",
       "      <td>265</td>\n",
       "      <td>2</td>\n",
       "      <td>881171488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>253</td>\n",
       "      <td>465</td>\n",
       "      <td>5</td>\n",
       "      <td>891628467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "1      186      302       3  891717742\n",
       "3      244       51       2  880606923\n",
       "5      298      474       4  884182806\n",
       "6      115      265       2  881171488\n",
       "7      253      465       5  891628467"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please don't change this cell\n",
    "\n",
    "df = pd.read_csv('ml-100k/u.data', names=['user_id', 'item_id', 'rating', 'timestamp'], sep='\\t')\n",
    "\n",
    "# obtain top 500 users and top 500 items\n",
    "user_ids = df.groupby('user_id').count().sort_values(by='rating', ascending=False).head(500).index\n",
    "item_ids = df.groupby('item_id').count().sort_values(by='rating', ascending=False).head(500).index\n",
    "df = df[(df['user_id'].isin(user_ids)) & (df['item_id'].isin(item_ids))]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly select one rating from each user as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please don't change this cell\n",
    "\n",
    "# remap user and item ID\n",
    "df['user_id'] = df.groupby('user_id').ngroup()\n",
    "df['item_id'] = df.groupby('item_id').ngroup()\n",
    "\n",
    "test_df = df.groupby('user_id').sample(1, random_state=1024)\n",
    "train_df = df[~df.index.isin(test_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of users: 500\n",
      "The number of items: 500\n",
      "Avg. # of rated Items/User: 129.914\n",
      "Density of data: 0.259828\n",
      "Ratings Range: 1 - 5\n"
     ]
    }
   ],
   "source": [
    "# Please don't change this cell\n",
    "\n",
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "avg_num = df.groupby('user_id').size().mean()\n",
    "density = df.shape[0] / (n_users * n_items)\n",
    "min_ratings = df.rating.min()\n",
    "max_ratings = df.rating.max()\n",
    "\n",
    "print(\"The number of users: {}\" .format(n_users))\n",
    "print(\"The number of items: {}\" .format(n_items))\n",
    "print(\"Avg. # of rated Items/User: {}\" .format(avg_num))\n",
    "print(\"Density of data: {}\" .format(density))\n",
    "print(\"Ratings Range: {} - {}\" .format(min_ratings, max_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct the rating matrix based on train_df:\n",
      "[[5. 3. 4. ... 0. 0. 0.]\n",
      " [4. 0. 0. ... 0. 0. 0.]\n",
      " [4. 3. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 5. 0. ... 0. 4. 0.]]\n",
      "Construct the rating matrix based on test_df:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Please don't change this cell\n",
    "\n",
    "# Convert the format of datasets to matrices\n",
    "# Train dataset\n",
    "df_zeros = pd.DataFrame({\n",
    "    'user_id': np.tile(np.arange(0, n_users), n_items), \n",
    "    'item_id': np.repeat(np.arange(0, n_items), n_users), \n",
    "    'rating': 0})\n",
    "train_ds = df_zeros.merge(train_df, \n",
    "                          how='left', \n",
    "                          on=['user_id', 'item_id']).fillna(0.).pivot_table(\n",
    "                              values='rating_y', \n",
    "                              index='user_id', \n",
    "                              columns='item_id').values\n",
    "                           \n",
    "# Test dataset\n",
    "test_ds = df_zeros.merge(test_df, \n",
    "                         how='left', \n",
    "                         on=['user_id', 'item_id']).fillna(0.).pivot_table(\n",
    "                             values='rating_y', \n",
    "                             index='user_id', \n",
    "                             columns='item_id').values\n",
    "\n",
    "print(\"Construct the rating matrix based on train_df:\")\n",
    "print(train_ds)\n",
    "\n",
    "print(\"Construct the rating matrix based on test_df:\")\n",
    "print(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please don't change this cell\n",
    "EPSILON = 1e-9\n",
    "\n",
    "def user_corr(imputed_train_ds):\n",
    "    '''\n",
    "    Function for calculating user's similarity\n",
    "    '''\n",
    "    active_user_pearson_corr = np.zeros((imputed_train_ds.shape[0], imputed_train_ds.shape[0]))\n",
    "\n",
    "    # Compute Pearson Correlation Coefficient of All Pairs of Users between active set and training dataset\n",
    "    for i, user_i_vec in enumerate(imputed_train_ds):\n",
    "        for j, user_j_vec in enumerate(imputed_train_ds):\n",
    "\n",
    "            # ratings corated by the current pair od users\n",
    "            mask_i = user_i_vec > 0\n",
    "            mask_j = user_j_vec > 0\n",
    "\n",
    "            # corrated item index, skip if there are no corrated ratings\n",
    "            corrated_index = np.intersect1d(np.where(mask_i), np.where(mask_j))\n",
    "            if len(corrated_index) == 0:\n",
    "                continue\n",
    "\n",
    "            # average value of user_i_vec and user_j_vec\n",
    "            mean_user_i = np.sum(user_i_vec) / (np.sum(np.clip(user_i_vec, 0, 1)) + EPSILON)\n",
    "            mean_user_j = np.sum(user_j_vec) / (np.sum(np.clip(user_j_vec, 0, 1)) + EPSILON)\n",
    "\n",
    "            # compute pearson corr\n",
    "            user_i_sub_mean = user_i_vec[corrated_index] - mean_user_i\n",
    "            user_j_sub_mean = user_j_vec[corrated_index] - mean_user_j\n",
    "\n",
    "            r_ui_sub_r_i_sq = np.square(user_i_sub_mean)\n",
    "            r_uj_sub_r_j_sq = np.square(user_j_sub_mean)\n",
    "\n",
    "            r_ui_sum_sqrt = np.sqrt(np.sum(r_ui_sub_r_i_sq))\n",
    "            r_uj_sum_sqrt = np.sqrt(np.sum(r_uj_sub_r_j_sq))\n",
    "\n",
    "            sim = np.sum(user_i_sub_mean * user_j_sub_mean) / (r_ui_sum_sqrt * r_uj_sum_sqrt + EPSILON)\n",
    "            active_user_pearson_corr[i][j] = sim\n",
    "\n",
    "    return active_user_pearson_corr\n",
    "\n",
    "def predict(test_ds, imputed_train_ds, user_corr, k=20):\n",
    "    '''\n",
    "    Function for predicting ratings in test_ds\n",
    "    '''\n",
    "\n",
    "    # Predicting ratings of test set\n",
    "    predicted_ds = np.zeros_like(test_ds)\n",
    "\n",
    "    for (i, j), rating in np.ndenumerate(test_ds):\n",
    "\n",
    "        if rating > 0:\n",
    "\n",
    "            # only predict ratings on test set\n",
    "            sim_user_ids = np.argsort(user_corr[i])[-1:-(k + 1):-1]\n",
    "\n",
    "            #==================user-based==================#\n",
    "            # the coefficient values of similar users\n",
    "            sim_val = user_corr[i][sim_user_ids]\n",
    "\n",
    "            # the average value of the current user's ratings\n",
    "            sim_users = imputed_train_ds[sim_user_ids]\n",
    "            \n",
    "            mask_rateditem_user = imputed_train_ds[i] != 0\n",
    "            num_rated_items = mask_rateditem_user.astype(np.float32)\n",
    "            user_mean = np.sum(imputed_train_ds[i, mask_rateditem_user]) / (num_rated_items.sum() + EPSILON)\n",
    "\n",
    "            mask_nei_rated_items = sim_users != 0\n",
    "            num_rated_per_user = mask_nei_rated_items.astype(np.float32)\n",
    "            num_per_user = num_rated_per_user.sum(axis=1)\n",
    "\n",
    "            sum_per_user = sim_users.sum(axis=1)\n",
    "            sim_user_mean = sum_per_user / (num_per_user + EPSILON)\n",
    "            \n",
    "            mask_rated_j = sim_users[:, j] > 0\n",
    "                            \n",
    "            # sim(u, v) * (r_vj - mean_v)\n",
    "            sim_r_sum_mean = sim_val[mask_rated_j] * (sim_users[mask_rated_j, j] - sim_user_mean[mask_rated_j])\n",
    "            \n",
    "            user_based_pred = user_mean + np.sum(sim_r_sum_mean) / (np.sum(sim_val[mask_rated_j]) + EPSILON)\n",
    "\n",
    "            predicted_ds[i, j] = np.clip(user_based_pred, 0, 5)\n",
    "            \n",
    "    return predicted_ds\n",
    "\n",
    "def evaluate(test_ds, predicted_ds):\n",
    "    '''\n",
    "    Function for evaluating on MAE and RMSE\n",
    "    '''\n",
    "    # MAE\n",
    "    mask_test_ds = test_ds > 0\n",
    "    MAE = np.sum(np.abs(test_ds[mask_test_ds] - predicted_ds[mask_test_ds])) / np.sum(mask_test_ds.astype(np.float32))\n",
    "\n",
    "    # RMSE\n",
    "    RMSE = np.sqrt(np.sum(np.square(test_ds[mask_test_ds] - predicted_ds[mask_test_ds])) / np.sum(mask_test_ds.astype(np.float32)))\n",
    "\n",
    "    return MAE, RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline - KNN based recommendation (Similarity Metric: Pearson Correlation Coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please don't change this cell\n",
    "\n",
    "user_pearson_corr = user_corr(train_ds)\n",
    "predicted_ds = predict(test_ds, train_ds, user_pearson_corr, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== Baseline Result =====================\n",
      "MAE: 0.8471711011333851, RMSE: 1.092846045041526\n"
     ]
    }
   ],
   "source": [
    "# Please don't change this cell\n",
    "\n",
    "MAE, RMSE = evaluate(test_ds, predicted_ds)\n",
    "\n",
    "print(\"===================== Baseline Result =====================\")\n",
    "print(\"MAE: {}, RMSE: {}\" .format(MAE, RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Solution\n",
    "(Put all your implementation for your solution in the following cell only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3    4    5    6    7    8    9    ...  490  491  492  \\\n",
      "0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "2    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "3    0.0  0.0  0.0  0.0  0.0  0.0  0.0  4.0  3.0  0.0  ...  0.0  0.0  0.0   \n",
      "4    0.0  0.0  4.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "495  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  5.0  0.0   \n",
      "496  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  4.0  ...  0.0  0.0  0.0   \n",
      "497  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "498  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "499  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "\n",
      "     493  494  495  496  497  498  499  \n",
      "0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..   ...  ...  ...  ...  ...  ...  ...  \n",
      "495  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "496  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "497  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "498  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "499  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[500 rows x 500 columns]\n",
      "[[ 1.          0.02133036 -0.02297143 ...  0.00416418  0.02343405\n",
      "   0.02032797]\n",
      " [ 0.02133036  1.          0.07258373 ...  0.01949185  0.01323848\n",
      "   0.03197385]\n",
      " [-0.02297143  0.07258373  1.         ...  0.04013809  0.05869343\n",
      "   0.07385776]\n",
      " ...\n",
      " [ 0.00416418  0.01949185  0.04013809 ...  1.          0.03044532\n",
      "  -0.00991878]\n",
      " [ 0.02343405  0.01323848  0.05869343 ...  0.03044532  1.\n",
      "   0.06050823]\n",
      " [ 0.02032797  0.03197385  0.07385776 ... -0.00991878  0.06050823\n",
      "   1.        ]]\n",
      "Composite Recommendation Value is 1884.3930635729228\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "\n",
    "#====================Split Dataset ===================\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "train_df, test_df\n",
    "\n",
    "# Training Dataset\n",
    "train_ds = np.zeros((n_users, n_items))\n",
    "for row in train_df.itertuples():\n",
    "    train_ds[row[1]-1, row[2]-1] = row[3]\n",
    "train_ds = pd.DataFrame(train_ds)\n",
    "\n",
    "# Testing Dataset\n",
    "test_ds = np.zeros((n_users, n_items))\n",
    "for row in test_df.itertuples():\n",
    "    test_ds[row[1]-1, row[2]-1] = row[3]\n",
    "test_ds = pd.DataFrame(test_ds)\n",
    "\n",
    "train_ds, test_ds\n",
    "\n",
    "print(test_ds)\n",
    "\n",
    "#======================IMPLEMENT THE EXISTING SOLUTION ===================\n",
    "\n",
    "# User-based\n",
    "# P(t) is item's popularity: P(t)=|U(t)| where U(t) is number of users rating on item t\n",
    "P = 500\n",
    "\n",
    "# m is the total numbers of users in database\n",
    "m= 1000\n",
    "\n",
    "# w(t) = log (m/P(t)) \n",
    "# popularity significance weight of item t :\n",
    "w = math.log (m/P)\n",
    "\n",
    "\n",
    "\n",
    "#================= Calculate the similarity  (centered cosine similarity) between every two users ===============\n",
    "\n",
    "#Gamma is for significant weighting\n",
    "GAMMA = 30\n",
    "\n",
    "#Epsilon is a very small number to prevent the error of division by zero\n",
    "EPSILON = 1e-9\n",
    "\n",
    "#define user-user similarity\n",
    "np_user_pearson_corr = np.zeros((n_users, n_users))\n",
    "\n",
    "#calcualate the similarity for every pair of users by using nested for loop statement\n",
    "for a, user_a_vec in enumerate(train_ds.values):\n",
    "    for u, user_u_vec in enumerate(train_ds.values):\n",
    "\n",
    "        # define the masks to find the set of items rated by users a and u, respectively\n",
    "        mask_a = user_a_vec > 0\n",
    "        mask_u = user_u_vec > 0\n",
    "\n",
    "        #find the co-rated items set which voted by user a or user u\n",
    "        corrated_index = np.union1d(np.where(mask_a), np.where(mask_u))\n",
    "        \n",
    "        # skip if there are no co-rated ratings a and u \n",
    "        # then leave the default zero as the similarity between a and u\n",
    "        if len(corrated_index) == 0:\n",
    "            continue\n",
    "      \n",
    "        # calculate average rating for row (user a and u)\n",
    "        mean_user_a = np.sum(user_a_vec) / (np.sum(np.clip(user_a_vec, 0, 1)) + EPSILON)\n",
    "        mean_user_u = np.sum(user_u_vec) / (np.sum(np.clip(user_u_vec, 0, 1)) + EPSILON)\n",
    "        \n",
    "        # perform the centered tranformation for ratings in the co-rated item set by subtracting the avergae rating of a and u, respectively\n",
    "        user_a_sub_mean = user_a_vec[corrated_index] - mean_user_a\n",
    "        user_u_sub_mean = user_u_vec[corrated_index] - mean_user_u\n",
    "        \n",
    "        # perform the centred cosine similarity betweeb user a and u\n",
    "        # perform the sqaure part\n",
    "        r_ua_sub_r_a_sq = np.square(user_a_sub_mean)\n",
    "        r_uu_sub_r_u_sq = np.square(user_u_sub_mean)\n",
    "        \n",
    "        # perform the square root part\n",
    "        r_ua_sum_sqrt = np.sqrt(np.sum(r_ua_sub_r_a_sq * w * w))\n",
    "        r_uu_sum_sqrt = np.sqrt(np.sum(r_uu_sub_r_u_sq * w * w))\n",
    "        \n",
    "        # perform similarity\n",
    "        sim = np.sum(w * w * user_a_sub_mean * user_u_sub_mean) / (r_ua_sum_sqrt * r_uu_sum_sqrt + EPSILON)\n",
    "\n",
    "        # calculate significance weighting\n",
    "        weighted_sim = (min(len(corrated_index), GAMMA) / GAMMA) * sim\n",
    "        np_user_pearson_corr[a][u] = weighted_sim\n",
    "np_user_pearson_corr\n",
    "\n",
    "print(np_user_pearson_corr)\n",
    "\n",
    "\n",
    "#========= EVALUATATE THE IMPLEMENTATION BY PREDICTIING THE RATINGS IN TEST SET (test_ds) ==========\n",
    "\n",
    "# define the prediction matrix to store the predicted ratings\n",
    "np_predictions = np.zeros((n_users, n_items))\n",
    "\n",
    "# K is the number of neighbours\n",
    "K = 20\n",
    "\n",
    "# Epsilon is a very small number to prevent the error of division by zero\n",
    "EPSILON = 1e-9\n",
    "\n",
    "# predict every rating in test_ds using the for loop statement \n",
    "for (a, u), rating in np.ndenumerate(test_ds.values):\n",
    "    if rating > 0:\n",
    "        \n",
    "        # find top-k most similar users as the current user, remove itself\n",
    "        sim_user_ids = np.argsort(np_user_pearson_corr[a])[-(K + 1):-1]\n",
    "        \n",
    "        # the actual similarity values for the top-k neighbours\n",
    "        sim_val = np_user_pearson_corr[a][sim_user_ids]\n",
    "        \n",
    "        #find the rows for the top-k neighbours\n",
    "        sim_users = train_ds.values[sim_user_ids]\n",
    "        \n",
    "        #calculate the average rating for the target user a, and the top-k neighbours (users)\n",
    "        user_mean = np.sum(train_ds.values[a]) / (np.sum(np.clip(train_ds.values[a], 0, 1)) + EPSILON)\n",
    "        \n",
    "        sim_user_mean = np.sum(sim_users, axis=1) / (np.sum(np.clip(sim_users, 0, 1), axis=1) + EPSILON)\n",
    "\n",
    "        # select the users who rated item j\n",
    "        mask_rated_u = sim_users[:, u] > 0\n",
    "        \n",
    "   \n",
    "        # calcualte the predicted rating: sim(a,u) * (r_ui - mean_u)\n",
    "        sim_r_sum_mean = sim_val[mask_rated_u] * (sim_users[mask_rated_u, u] - sim_user_mean[mask_rated_u])\n",
    "        \n",
    "        np_predictions[a][u] = user_mean + np.sum(sim_r_sum_mean) / (np.sum(sim_val[mask_rated_u]) + EPSILON)\n",
    "        \n",
    "        # clip the predicted rating into the correct range of the ratings (from 0 to 5)\n",
    "        np_predictions[a][u] = np.clip(np_predictions[a][u], 0, 5)\n",
    "        \n",
    "    np_predictions[a][u]    \n",
    "\n",
    "#========== generate the recommendation items list for user a, based on the Top-N highest prediction value======\n",
    "\n",
    "# define a composite recommendation value for every item t\n",
    "# ComRV(t) = P(t) * mean_R(t)\n",
    "comRV= 500 * mean_user_a\n",
    "print(\"Composite Recommendation Value is\", comRV)\n",
    "\n",
    "    \n",
    "\n",
    "#================== EVALUATE THE PREDICTION BY USING MAE =====================\n",
    "\n",
    "labels = test_ds.values    \n",
    "\n",
    "#calculate the absoblute error between every prediction and every rating correspondingly\n",
    "absolute_error = np.abs(np_predictions - labels)\n",
    "\n",
    "# clip the rating in the labels (test set) into 1 and 0\n",
    "# then use this to  calculate the number of rating in the test set\n",
    "weight = np.clip(labels, 0, 1)\n",
    "\n",
    "# absoulte error on rated ratings\n",
    "abs_error = absolute_error * weight\n",
    "\n",
    "# calcualte MAE\n",
    "MAE = np.sum(abs_error) / np.sum(weight)\n",
    "\n",
    "\n",
    "\n",
    "#================= EVALUATE THE PREDICTION BY USING RMSE ======================\n",
    "labels = test_ds.values\n",
    "\n",
    "# calculate squared values for evry prediction and every rating (in the test set) correspondingly. \n",
    "squared_error = np.square(np_predictions - labels)\n",
    "\n",
    "# clip the rating in the labels (test set) into 1 and 0\n",
    "# then use this to  calculate the number of rating in the test set\n",
    "weight = np.clip(labels, 0, 1)\n",
    "\n",
    "# squared error on rated ratings\n",
    "squared_error = squared_error * weight\n",
    "\n",
    "# calculate RMSE\n",
    "RMSE = np.sqrt(np.sum(squared_error) / np.sum(weight))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the MAE and RMSE of Your Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== The MAE and RMSE of Your Implementation =====================\n",
      "MAE: 0.7654304847345743, RMSE: 0.9846462286564435\n"
     ]
    }
   ],
   "source": [
    "# Please don't change this cell\n",
    "\n",
    "print(\"===================== The MAE and RMSE of Your Implementation =====================\")\n",
    "print(\"MAE: {}, RMSE: {}\" .format(MAE, RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
